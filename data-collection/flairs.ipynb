{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7283f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6138ab28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2996, 6)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv for all universities\n",
    "df = pd.read_csv('all_posts.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c422a7",
   "metadata": {},
   "source": [
    "Social media tends to have a lot of memes/non-sensical discussion. For this project it would be valuable to filter out these types of posts and focus on posts that are more likely to have criticisms about the university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06cf5d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Transfers', 'Discussion', 'Humour', 'Serious', 'News', 'Other',\n",
       "       'Rant', 'Health', 'Clubs/Sports', 'Social', 'Event', 'Courses',\n",
       "       'Waterloo #173', 'Academics', 'Advice', 'Confirmed', 'Meta', nan,\n",
       "       'Shitpost', 'Lost & Found', 'Question', 'Politics', 'Life Advice',\n",
       "       'Admissions', 'Finances', 'Programs', 'ACORN/Quercus/Outlook',\n",
       "       'Free Speech', 'Waterloo #201‚Äì250', 'UTM/UTSC',\n",
       "       \"I'm in High School\", 'Jobs', 'Photography & Art',\n",
       "       'Pho(ur seasons)tography & Art', '@ SFU (Exception)', 'Megathread',\n",
       "       'Congrats, you made it!', 'Photography &amp; Art',\n",
       "       'Humour - Satire', 'SFU = Studying For UBC', 'Prose', 'üçÅ',\n",
       "       '100% super duper confirmed by the r/byssey', 'üî•üî•üî•',\n",
       "       'Ghost-type Humour', 'Unverified', 'Lost Dog', 'üéâüéâüéâ',\n",
       "       'Read Comments Section for full context', 'Missing Person', 'F',\n",
       "       'Spicy', 'HQ Post', 'Spicy Meme', 'We did it, reddit!',\n",
       "       'HQ shitpost', 'Certified Dank', 'shitpost', 'Political',\n",
       "       'I should be working rn', 'MEGATHREAD', 'University Politics',\n",
       "       'TW: sexual violence', 'From Laurier University', 'TW: Suicide',\n",
       "       'Martlet Club', 'coronavirus', 'HQ Shitpost', 'Wholesome Post',\n",
       "       'artsy shitpost', 'SSMU', 'megathread', 'Resolved! ',\n",
       "       'TW: sexual harrassment', 'Stay Home', 'Funny/Meme'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to see post flairs, and subsequently filter for non-shitposts\n",
    "df['flair'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b812338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flair\n",
       "Humour               962\n",
       "Discussion           223\n",
       "Other                159\n",
       "Photography & Art    116\n",
       "shitpost              73\n",
       "News                  49\n",
       "Academics             28\n",
       "Advice                22\n",
       "Courses               22\n",
       "HQ Post               21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most popular flairs\n",
    "df['flair'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ab0ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 6)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['flair'].isin(['Discussion', 'News', 'Academics', 'Advice', 'Courses', 'HQ Post'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d7c10",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "977dbecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_created</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>flair</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-08 03:24:09</td>\n",
       "      <td>[ Removed by Reddit ]</td>\n",
       "      <td>[ Removed by Reddit on account of violating th...</td>\n",
       "      <td>['Ik this man, he‚Äôs getting cooked. Maybe uoft...</td>\n",
       "      <td>2929</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>['ik this man, he‚Äôs getting cooked. maybe uoft...</td>\n",
       "      <td>[man, getting, cooked, maybe, uoft, anything, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-14 00:11:02</td>\n",
       "      <td>University of Toronto Faculty Association vote...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Big W, honestly im surprised', 'W faculty', ...</td>\n",
       "      <td>2157</td>\n",
       "      <td>News</td>\n",
       "      <td>uoft faculty association votes to divest from ...</td>\n",
       "      <td>[uoft, faculty, association, vote, divest, isr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-06-03 21:51:09</td>\n",
       "      <td>Just graduated at Convocation with encampment ...</td>\n",
       "      <td>Just graduated and guess what, the encampment ...</td>\n",
       "      <td>[\"that's so good to hear! i'm a huge supporter...</td>\n",
       "      <td>1612</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>just graduated at convocation with encampment ...</td>\n",
       "      <td>[graduated, convocation, encampment, present, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-12-19 14:19:36</td>\n",
       "      <td>Is this MAT224 final average fr? (not my class...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['overconfident library vegetable dinosaurs ab...</td>\n",
       "      <td>1284</td>\n",
       "      <td>Courses</td>\n",
       "      <td>is this mat224 final average fr? (not my class...</td>\n",
       "      <td>[final, average, fr, class, friend, sent, libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-07-04 00:49:45</td>\n",
       "      <td>All that‚Äôs left of the encampment after todays...</td>\n",
       "      <td>Photo by @a1please on instagram</td>\n",
       "      <td>['the grass is fucked üò≠', \"Out of the loop, wh...</td>\n",
       "      <td>1271</td>\n",
       "      <td>News</td>\n",
       "      <td>all that‚Äôs left of the encampment after todays...</td>\n",
       "      <td>[left, encampment, today, event, photo, instag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date_created                                              title  \\\n",
       "1   2024-11-08 03:24:09                              [ Removed by Reddit ]   \n",
       "4   2025-05-14 00:11:02  University of Toronto Faculty Association vote...   \n",
       "13  2024-06-03 21:51:09  Just graduated at Convocation with encampment ...   \n",
       "22  2023-12-19 14:19:36  Is this MAT224 final average fr? (not my class...   \n",
       "27  2024-07-04 00:49:45  All that‚Äôs left of the encampment after todays...   \n",
       "\n",
       "                                          description  \\\n",
       "1   [ Removed by Reddit on account of violating th...   \n",
       "4                                                 NaN   \n",
       "13  Just graduated and guess what, the encampment ...   \n",
       "22                                                NaN   \n",
       "27                   Photo by @a1please on instagram    \n",
       "\n",
       "                                             comments  upvotes       flair  \\\n",
       "1   ['Ik this man, he‚Äôs getting cooked. Maybe uoft...     2929  Discussion   \n",
       "4   ['Big W, honestly im surprised', 'W faculty', ...     2157        News   \n",
       "13  [\"that's so good to hear! i'm a huge supporter...     1612  Discussion   \n",
       "22  ['overconfident library vegetable dinosaurs ab...     1284     Courses   \n",
       "27  ['the grass is fucked üò≠', \"Out of the loop, wh...     1271        News   \n",
       "\n",
       "                                        combined_text  \\\n",
       "1   ['ik this man, he‚Äôs getting cooked. maybe uoft...   \n",
       "4   uoft faculty association votes to divest from ...   \n",
       "13  just graduated at convocation with encampment ...   \n",
       "22  is this mat224 final average fr? (not my class...   \n",
       "27  all that‚Äôs left of the encampment after todays...   \n",
       "\n",
       "                                    lemmatized_tokens  \n",
       "1   [man, getting, cooked, maybe, uoft, anything, ...  \n",
       "4   [uoft, faculty, association, vote, divest, isr...  \n",
       "13  [graduated, convocation, encampment, present, ...  \n",
       "22  [final, average, fr, class, friend, sent, libr...  \n",
       "27  [left, encampment, today, event, photo, instag...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(df):\n",
    "    '''\n",
    "    This function does 3 things\n",
    "\n",
    "    1. Combines text from title, description and comments\n",
    "    2. Normalizes university names\n",
    "    3. Tokenizes and lemmatizes text and removes stopwords + reddit specific words\n",
    "\n",
    "    '''\n",
    "    remove_words = ['removed', 'deleted', '[ removed by reddit ]', '[deleted]']\n",
    "    # Combine text from title, description and comments\n",
    "    def combine_text(text): \n",
    "\n",
    "        if isinstance(text, list):\n",
    "            return ' '.join([word for word in text if isinstance(word, str) and word.lower() not in remove_words])\n",
    "        elif isinstance(text, str): \n",
    "            # Filter out posts removed by reddit\n",
    "            if re.search(r\"\\[?\\s*removed by reddit.*?\\]?\", text, flags=re.IGNORECASE):\n",
    "                return ''\n",
    "            return text\n",
    "        return ''\n",
    "\n",
    "    \n",
    "    \n",
    "    # Normalize university names\n",
    "    def normalize_university(text):\n",
    "        # Regex to normalize uni names\n",
    "        text = re.sub(r'u\\sof\\st', 'uoft', text)\n",
    "        text = re.sub(r'university of toronto', 'uoft', text)\n",
    "        text = re.sub(r'university of british columbia', 'ubc', text)\n",
    "        return text\n",
    "\n",
    "    # Tokenize + lematize + remove stopwords\n",
    "    def tokenize_and_lemmatize(text): \n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Filter for strings\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Combine text\n",
    "    df['combined_text'] = (df['title'].apply(combine_text).fillna('') + ' ' +\n",
    "                       df['description'].apply(combine_text).fillna('') + ' ' +\n",
    "                       df['comments'].apply(combine_text))\n",
    "\n",
    "    # Lower + removing trailing charsa\n",
    "    df['combined_text'] = df['combined_text'].str.lower().str.strip()\n",
    "    \n",
    "    # Normalize university names\n",
    "    df['combined_text'] = df['combined_text'].apply(normalize_university)\n",
    "\n",
    "    # Tokenize + lemmatize\n",
    "    df['lemmatized_tokens'] = df['combined_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "    # Return cleaned df\n",
    "    return df\n",
    "\n",
    "df = clean_text(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0bb6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = df[df['flair'] == 'Discussion']['lemmatized_tokens'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
