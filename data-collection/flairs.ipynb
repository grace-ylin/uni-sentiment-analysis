{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7283f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6138ab28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2996, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv for all universities\n",
    "df = pd.read_csv('all_posts.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c422a7",
   "metadata": {},
   "source": [
    "Social media tends to have a lot of memes/non-sensical discussion. For this project it would be valuable to filter out these types of posts and focus on posts that are more likely to have criticisms about the university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06cf5d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Transfers', 'Discussion', 'Humour', 'Serious', 'News', 'Other',\n",
       "       'Rant', 'Health', 'Clubs/Sports', 'Social', 'Event', 'Courses',\n",
       "       'Waterloo #173', 'Academics', 'Advice', 'Confirmed', 'Meta', nan,\n",
       "       'Shitpost', 'Lost & Found', 'Question', 'Politics', 'Life Advice',\n",
       "       'Admissions', 'Finances', 'Programs', 'ACORN/Quercus/Outlook',\n",
       "       'Free Speech', 'Waterloo #201‚Äì250', 'UTM/UTSC',\n",
       "       \"I'm in High School\", 'Jobs', 'Photography & Art',\n",
       "       'Pho(ur seasons)tography & Art', '@ SFU (Exception)', 'Megathread',\n",
       "       'Congrats, you made it!', 'Photography &amp; Art',\n",
       "       'Humour - Satire', 'SFU = Studying For UBC', 'Prose', 'üçÅ',\n",
       "       '100% super duper confirmed by the r/byssey', 'üî•üî•üî•',\n",
       "       'Ghost-type Humour', 'Unverified', 'Lost Dog', 'üéâüéâüéâ',\n",
       "       'Read Comments Section for full context', 'Missing Person', 'F',\n",
       "       'Spicy', 'HQ Post', 'Spicy Meme', 'We did it, reddit!',\n",
       "       'HQ shitpost', 'Certified Dank', 'shitpost', 'Political',\n",
       "       'I should be working rn', 'MEGATHREAD', 'University Politics',\n",
       "       'TW: sexual violence', 'From Laurier University', 'TW: Suicide',\n",
       "       'Martlet Club', 'coronavirus', 'HQ Shitpost', 'Wholesome Post',\n",
       "       'artsy shitpost', 'SSMU', 'megathread', 'Resolved! ',\n",
       "       'TW: sexual harrassment', 'Stay Home', 'Funny/Meme'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to see post flairs, and subsequently filter for non-shitposts\n",
    "df['flair'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b812338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flair\n",
       "Humour               961\n",
       "Discussion           223\n",
       "Other                159\n",
       "Photography & Art    116\n",
       "shitpost              73\n",
       "News                  49\n",
       "Academics             28\n",
       "Advice                22\n",
       "Courses               22\n",
       "HQ Post               21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most popular flairs\n",
    "df['flair'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ab0ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['flair'].isin(['Discussion', 'News', 'Academics', 'Advice', 'Courses', 'HQ Post'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d7c10",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dbecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_created</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>flair</th>\n",
       "      <th>university</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-12 18:36:14</td>\n",
       "      <td>To the student who got caught using AI on thei...</td>\n",
       "      <td>Thanks a lot you dumb f*ck. Due to your idiocy...</td>\n",
       "      <td>['[deleted]', 'It‚Äôs crazy that there‚Äôs people ...</td>\n",
       "      <td>2942</td>\n",
       "      <td>Transfers</td>\n",
       "      <td>UofT</td>\n",
       "      <td>to the student who got caught using ai on thei...</td>\n",
       "      <td>[student, got, caught, using, ai, exam, uoft, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-08 03:24:09</td>\n",
       "      <td>[ Removed by Reddit ]</td>\n",
       "      <td>[ Removed by Reddit on account of violating th...</td>\n",
       "      <td>['Ik this man, he‚Äôs getting cooked. Maybe uoft...</td>\n",
       "      <td>2939</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>UofT</td>\n",
       "      <td>['ik this man, he‚Äôs getting cooked. maybe uoft...</td>\n",
       "      <td>[man, getting, cooked, maybe, uoft, anything, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-13 19:48:06</td>\n",
       "      <td>I want a ps5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['i LOVE  how low effort this looks', 'pain', ...</td>\n",
       "      <td>2749</td>\n",
       "      <td>Humour</td>\n",
       "      <td>UofT</td>\n",
       "      <td>i want a ps5  ['i love  how low effort this lo...</td>\n",
       "      <td>[want, love, low, effort, look, u, made, day, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-14 00:38:33</td>\n",
       "      <td>If this thread gets over 300 upvotes in the ne...</td>\n",
       "      <td>srsly don't fuck this up for me, we're already...</td>\n",
       "      <td>['[deleted]', '[deleted]', \"Found this in /r/a...</td>\n",
       "      <td>2485</td>\n",
       "      <td>Serious</td>\n",
       "      <td>UofT</td>\n",
       "      <td>if this thread gets over 300 upvotes in the ne...</td>\n",
       "      <td>[thread, get, upvotes, next, change, sub, exam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-14 00:11:02</td>\n",
       "      <td>University of Toronto Faculty Association vote...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Big W, honestly im surprised', 'W faculty', ...</td>\n",
       "      <td>2163</td>\n",
       "      <td>News</td>\n",
       "      <td>UofT</td>\n",
       "      <td>uoft faculty association votes to divest from ...</td>\n",
       "      <td>[uoft, faculty, association, vote, divest, isr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_created                                              title  \\\n",
       "0  2025-02-12 18:36:14  To the student who got caught using AI on thei...   \n",
       "1  2024-11-08 03:24:09                              [ Removed by Reddit ]   \n",
       "2  2020-11-13 19:48:06                                       I want a ps5   \n",
       "3  2016-12-14 00:38:33  If this thread gets over 300 upvotes in the ne...   \n",
       "4  2025-05-14 00:11:02  University of Toronto Faculty Association vote...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Thanks a lot you dumb f*ck. Due to your idiocy...   \n",
       "1  [ Removed by Reddit on account of violating th...   \n",
       "2                                                NaN   \n",
       "3  srsly don't fuck this up for me, we're already...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            comments  upvotes       flair  \\\n",
       "0  ['[deleted]', 'It‚Äôs crazy that there‚Äôs people ...     2942   Transfers   \n",
       "1  ['Ik this man, he‚Äôs getting cooked. Maybe uoft...     2939  Discussion   \n",
       "2  ['i LOVE  how low effort this looks', 'pain', ...     2749      Humour   \n",
       "3  ['[deleted]', '[deleted]', \"Found this in /r/a...     2485     Serious   \n",
       "4  ['Big W, honestly im surprised', 'W faculty', ...     2163        News   \n",
       "\n",
       "  university                                      combined_text  \\\n",
       "0       UofT  to the student who got caught using ai on thei...   \n",
       "1       UofT  ['ik this man, he‚Äôs getting cooked. maybe uoft...   \n",
       "2       UofT  i want a ps5  ['i love  how low effort this lo...   \n",
       "3       UofT  if this thread gets over 300 upvotes in the ne...   \n",
       "4       UofT  uoft faculty association votes to divest from ...   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [student, got, caught, using, ai, exam, uoft, ...  \n",
       "1  [man, getting, cooked, maybe, uoft, anything, ...  \n",
       "2  [want, love, low, effort, look, u, made, day, ...  \n",
       "3  [thread, get, upvotes, next, change, sub, exam...  \n",
       "4  [uoft, faculty, association, vote, divest, isr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.add('would')\n",
    "stop_words.add('just')\n",
    "stop_words.add('also')\n",
    "\n",
    "def clean_text(df):\n",
    "    '''\n",
    "    This function does 3 things\n",
    "\n",
    "    1. Combines text from title, description and comments\n",
    "    2. Normalizes university names\n",
    "    3. Tokenizes and lemmatizes text and removes stopwords + reddit specific words\n",
    "\n",
    "    '''\n",
    "    remove_words = ['removed', 'deleted', '[ removed by reddit ]', '[deleted]']\n",
    "    # custom_stopwords = {'student', 'course', 'people', 'get', 'like', 'time', 'year', 'would'}\n",
    "    # Combine text from title, description and comments\n",
    "    def combine_text(text): \n",
    "\n",
    "        if isinstance(text, list):\n",
    "            return ' '.join([word for word in text if isinstance(word, str) and word.lower() not in remove_words])\n",
    "        elif isinstance(text, str): \n",
    "            # Filter out posts removed by reddit\n",
    "            if re.search(r\"\\[?\\s*removed by reddit.*?\\]?\", text, flags=re.IGNORECASE):\n",
    "                return ''\n",
    "            return text\n",
    "        return ''\n",
    "\n",
    "    \n",
    "    \n",
    "    # Normalize university names\n",
    "    def normalize_university(text):\n",
    "        # Regex to normalize uni names\n",
    "        text = re.sub(r'u\\sof\\st', 'uoft', text)\n",
    "        text = re.sub(r'university of toronto', 'uoft', text)\n",
    "        text = re.sub(r'university of british columbia', 'ubc', text)\n",
    "        return text\n",
    "\n",
    "    # Tokenize + lematize + remove stopwords\n",
    "    def tokenize_and_lemmatize(text): \n",
    "\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Filter for strings\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Combine text\n",
    "    df['combined_text'] = (df['title'].apply(combine_text).fillna('') + ' ' +\n",
    "                       df['description'].apply(combine_text).fillna('') + ' ' +\n",
    "                       df['comments'].apply(combine_text))\n",
    "\n",
    "    # Lower + removing trailing charsa\n",
    "    df['combined_text'] = df['combined_text'].str.lower().str.strip()\n",
    "    \n",
    "    # Normalize university names\n",
    "    df['combined_text'] = df['combined_text'].apply(normalize_university)\n",
    "\n",
    "    # Tokenize + lemmatize\n",
    "    df['lemmatized_tokens'] = df['combined_text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "    # Return cleaned df\n",
    "    return df\n",
    "\n",
    "df = clean_text(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a48a2",
   "metadata": {},
   "source": [
    "## Investigating top k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ff2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---All Posts---\n",
      "[('people', 15312), ('like', 14454), ('student', 13110), ('get', 11587), ('year', 9617), ('one', 9602), ('time', 8748), ('think', 8472), ('know', 7732), ('even', 7111)]\n",
      "---Transfers---\n",
      "[('ai', 60), ('college', 52), ('lawyer', 34), ('use', 32), ('law', 30), ('school', 28), ('university', 28), ('student', 26), ('using', 21), ('people', 21)]\n",
      "---Discussion---\n",
      "[('people', 3817), ('like', 3058), ('student', 2560), ('get', 2262), ('one', 1875), ('year', 1851), ('think', 1815), ('time', 1560), ('know', 1559), ('even', 1550)]\n",
      "---Humour---\n",
      "[('like', 2250), ('get', 1882), ('year', 1841), ('people', 1606), ('one', 1567), ('student', 1558), ('course', 1456), ('time', 1429), ('class', 1133), ('deleted', 1090)]\n",
      "---Serious---\n",
      "[('http', 21), ('deleted', 16), ('get', 12), ('goose', 11), ('meme', 10), ('u', 10), ('post', 9), ('hour', 9), ('uoft', 9), ('upvotes', 8)]\n",
      "---News---\n",
      "[('people', 699), ('student', 545), ('like', 378), ('get', 358), ('one', 328), ('think', 313), ('uoft', 269), ('know', 269), ('http', 267), ('even', 247)]\n",
      "---Other---\n",
      "[('like', 749), ('people', 556), ('year', 533), ('one', 527), ('get', 517), ('student', 495), ('think', 436), ('time', 433), ('uoft', 362), ('know', 346)]\n",
      "---Rant---\n",
      "[('uoft', 293), ('student', 240), ('like', 196), ('university', 171), ('get', 153), ('school', 136), ('people', 134), ('year', 126), ('one', 112), ('time', 109)]\n",
      "---Health---\n",
      "[('people', 213), ('removed', 188), ('like', 148), ('get', 138), ('time', 115), ('think', 101), ('mask', 100), ('one', 99), ('know', 99), ('year', 87)]\n",
      "---Clubs/Sports---\n",
      "[('walk', 34), ('bike', 27), ('keyboard', 19), ('next', 18), ('like', 17), ('student', 15), ('love', 14), ('bikechain', 14), ('http', 14), ('get', 13)]\n",
      "---Social---\n",
      "[('student', 118), ('english', 76), ('international', 66), ('people', 35), ('class', 34), ('uoft', 33), ('language', 30), ('canadian', 30), ('even', 29), ('like', 27)]\n",
      "---Event---\n",
      "[('people', 74), ('like', 57), ('chalkboard', 43), ('board', 32), ('ukrainian', 32), ('one', 30), ('get', 29), ('ukraine', 28), ('rally', 26), ('help', 22)]\n",
      "---Courses---\n",
      "[('course', 336), ('student', 262), ('like', 218), ('class', 211), ('time', 149), ('one', 145), ('year', 144), ('prof', 143), ('get', 133), ('grade', 114)]\n",
      "---Waterloo #173---\n",
      "[('post', 19), ('tattoo', 13), ('get', 10), ('http', 9), ('goose', 8), ('stupid', 8), ('meme', 7), ('truck', 7), ('food', 6), ('updoots', 6)]\n",
      "---Academics---\n",
      "[('student', 374), ('course', 234), ('exam', 194), ('get', 181), ('people', 172), ('like', 161), ('prof', 161), ('one', 143), ('time', 129), ('think', 121)]\n",
      "---Advice---\n",
      "[('year', 765), ('time', 598), ('get', 465), ('people', 402), ('like', 397), ('course', 369), ('take', 328), ('student', 326), ('work', 319), ('one', 318)]\n",
      "---Confirmed---\n",
      "[('police', 13), ('like', 10), ('guy', 9), ('hope', 9), ('asian', 9), ('caught', 8), ('got', 8), ('report', 7), ('deleted', 7), ('shit', 7)]\n",
      "---Meta---\n",
      "[('http', 83), ('uoft', 53), ('chinese', 49), ('subreddit', 46), ('post', 43), ('people', 40), ('student', 38), ('deleted', 38), ('kong', 35), ('university', 31)]\n",
      "---nan---\n",
      "[]\n",
      "---Shitpost---\n",
      "[('guy', 25), ('bucket', 23), ('like', 23), ('shit', 22), ('shall', 16), ('people', 15), ('man', 15), ('uoft', 15), ('someone', 14), ('one', 12)]\n",
      "---Lost & Found---\n",
      "[('back', 8), ('person', 4), ('get', 4), ('bro', 4), ('got', 4), ('ear', 4), ('week', 4), ('put', 3), ('address', 3), ('find', 3)]\n",
      "---Question---\n",
      "[('ci', 261), ('people', 144), ('like', 101), ('student', 82), ('go', 79), ('know', 79), ('year', 77), ('say', 74), ('one', 74), ('uoft', 67)]\n",
      "---Politics---\n",
      "[('student', 25), ('like', 21), ('online', 20), ('class', 17), ('instructor', 16), ('uoft', 13), ('university', 13), ('people', 11), ('take', 11), ('school', 11)]\n",
      "---Life Advice---\n",
      "[('year', 15), ('proud', 8), ('u', 7), ('took', 5), ('graduate', 5), ('major', 5), ('back', 5), ('much', 5), ('congrats', 5), ('got', 5)]\n",
      "---Admissions---\n",
      "[('got', 337), ('offer', 326), ('get', 305), ('admission', 300), ('round', 295), ('uoft', 246), ('application', 210), ('c', 203), ('still', 192), ('know', 185)]\n",
      "---Finances---\n",
      "[('student', 109), ('get', 70), ('year', 51), ('income', 49), ('job', 47), ('deleted', 47), ('make', 45), ('month', 44), ('still', 42), ('benefit', 42)]\n",
      "---Programs---\n",
      "[('black', 53), ('course', 44), ('people', 42), ('student', 22), ('like', 21), ('class', 15), ('point', 14), ('white', 14), ('right', 14), ('get', 13)]\n",
      "---ACORN/Quercus/Outlook---\n",
      "[('schedule', 22), ('year', 19), ('timetable', 14), ('like', 13), ('one', 13), ('course', 12), ('good', 11), ('na', 10), ('gon', 9), ('colour', 9)]\n",
      "---Free Speech---\n",
      "[('people', 55), ('http', 39), ('think', 26), ('deleted', 26), ('like', 25), ('speech', 22), ('student', 19), ('see', 18), ('peterson', 17), ('get', 16)]\n",
      "---Waterloo #201‚Äì250---\n",
      "[('cobra', 4), ('chicken', 4), ('e', 4), ('uwaterloo', 2), ('agent', 2), ('back', 2), ('never', 2), ('power', 2), ('ranking', 2), ('meme', 2)]\n",
      "---UTM/UTSC---\n",
      "[('bad', 4), ('look', 3), ('u', 3), ('danny', 2), ('utm', 2), ('panam', 2), ('know', 2), ('deleted', 2), ('comment', 2), ('cuz', 2)]\n",
      "---I'm in High School---\n",
      "[('get', 22), ('school', 18), ('high', 17), ('student', 12), ('c', 11), ('good', 11), ('applying', 9), ('year', 9), ('got', 9), ('uoft', 8)]\n",
      "---Jobs---\n",
      "[('job', 16), ('application', 12), ('gpa', 10), ('even', 9), ('cr', 9), ('get', 8), ('one', 8), ('cover', 8), ('letter', 8), ('like', 8)]\n",
      "---Photography & Art---\n",
      "[('ubc', 189), ('like', 174), ('year', 161), ('http', 151), ('time', 141), ('get', 139), ('one', 133), ('campus', 131), ('see', 124), ('good', 118)]\n",
      "---Pho(ur seasons)tography & Art---\n",
      "[('miss', 4), ('season', 3), ('picture', 3), ('walking', 2), ('school', 2), ('one', 2), ('left', 2), ('accurate', 2), ('got', 2), ('ubc', 2)]\n",
      "---@ SFU (Exception)---\n",
      "[('myanmar', 7), ('post', 7), ('ubc', 6), ('student', 6), ('hope', 5), ('piazza', 4), ('get', 4), ('deleted', 4), ('situation', 4), ('country', 3)]\n",
      "---Megathread---\n",
      "[('course', 316), ('year', 316), ('student', 289), ('get', 259), ('people', 240), ('ubc', 235), ('like', 206), ('science', 201), ('got', 180), ('class', 160)]\n",
      "---Congrats, you made it!---\n",
      "[('grade', 2), ('class', 2), ('average', 2), ('acted', 2), ('integrity', 2), ('deleted', 2), ('gotten', 2), ('even', 2), ('get', 2), ('cpen', 2)]\n",
      "---Photography &amp; Art---\n",
      "[('ubc', 5), ('national', 3), ('royale', 2), ('uni', 2), ('ranked', 2), ('u', 2), ('remain', 2), ('open', 2), ('world', 2), ('emergency', 2)]\n",
      "---Humour - Satire---\n",
      "[('mask', 12), ('deleted', 6), ('u', 5), ('wear', 4), ('http', 4), ('first', 3), ('know', 3), ('„ÉÑ', 3), ('important', 3), ('gon', 2)]\n",
      "---SFU = Studying For UBC---\n",
      "[('ndp', 14), ('vote', 10), ('riding', 9), ('voting', 9), ('ubc', 7), ('liberal', 5), ('conservative', 5), ('sfu', 4), ('look', 4), ('see', 4)]\n",
      "---Prose---\n",
      "[('ubc', 15), ('year', 14), ('school', 14), ('like', 11), ('beautiful', 11), ('place', 10), ('life', 10), ('feel', 9), ('experience', 9), ('goldenrod', 8)]\n",
      "---üçÅ---\n",
      "[('canada', 7), ('le', 3), ('canadian', 3), ('vive', 3), ('gooo', 2), ('mile', 2), ('go', 2), ('take', 2), ('game', 2), ('territory', 2)]\n",
      "---100% super duper confirmed by the r/byssey---\n",
      "[('joke', 10), ('april', 8), ('year', 6), ('mean', 5), ('fool', 4), ('day', 4), ('deleted', 4), ('people', 3), ('going', 3), ('think', 3)]\n",
      "---üî•üî•üî•---\n",
      "[('proctorio', 41), ('student', 41), ('exam', 26), ('academic', 26), ('faculty', 20), ('concern', 16), ('undergraduate', 14), ('society', 14), ('ubc', 14), ('vp', 14)]\n",
      "---Ghost-type Humour---\n",
      "[('cube', 3), ('place', 3), ('know', 3), ('go', 3), ('wooden', 2), ('always', 2), ('people', 2), ('pretty', 2), ('great', 2), ('inside', 2)]\n",
      "---Unverified---\n",
      "[('people', 58), ('like', 24), ('athlete', 23), ('ubc', 21), ('party', 21), ('team', 21), ('year', 20), ('variant', 19), ('think', 19), ('get', 18)]\n",
      "---Lost Dog---\n",
      "[('dog', 17), ('missing', 12), ('find', 10), ('continue', 10), ('ubc', 9), ('please', 9), ('search', 9), ('thank', 8), ('hope', 8), ('tofino', 7)]\n",
      "---üéâüéâüéâ---\n",
      "[('exam', 15), ('course', 6), ('student', 6), ('prof', 6), ('everyone', 5), ('class', 5), ('good', 5), ('one', 5), ('luck', 5), ('like', 5)]\n",
      "---Read Comments Section for full context---\n",
      "[('deleted', 23), ('proctorio', 21), ('student', 12), ('make', 10), ('get', 10), ('minute', 10), ('roy', 9), ('op', 9), ('like', 9), ('company', 9)]\n",
      "---Missing Person---\n",
      "[('hope', 4), ('find', 3), ('people', 3), ('friend', 3), ('soon', 3), ('pretty', 2), ('hiking', 2), ('season', 2), ('missing', 2), ('please', 2)]\n",
      "---F---\n",
      "[('f', 135), ('gerts', 12), ('year', 8), ('deleted', 7), ('next', 4), ('got', 4), ('day', 3), ('bit', 3), ('lost', 3), ('bda', 3)]\n",
      "---Spicy---\n",
      "[('indigenous', 68), ('people', 54), ('student', 48), ('admission', 34), ('ubc', 32), ('get', 32), ('person', 27), ('someone', 26), ('lower', 25), ('comment', 24)]\n",
      "---HQ Post---\n",
      "[('people', 121), ('like', 79), ('time', 62), ('mcgill', 61), ('year', 57), ('know', 55), ('one', 55), ('building', 52), ('get', 50), ('student', 49)]\n",
      "---Spicy Meme---\n",
      "[('turn', 4), ('camera', 3), ('mic', 3), ('zoom', 3), ('important', 2), ('take', 2), ('bathroom', 2), ('mode', 2), ('everyone', 2), ('went', 2)]\n",
      "---We did it, reddit!---\n",
      "[('community', 2), ('number', 2), ('accomplished', 1), ('burnside', 1), ('thank', 1), ('much', 1), ('whole', 1), ('making', 1), ('possible', 1), ('people', 1)]\n",
      "---HQ shitpost---\n",
      "[('squirrel', 6), ('ragan', 6), ('one', 5), ('shitpost', 4), ('final', 4), ('deleted', 3), ('back', 3), ('year', 3), ('actually', 3), ('thing', 3)]\n",
      "---Certified Dank---\n",
      "[('http', 4), ('content', 2), ('mb', 2), ('many', 1), ('assignment', 1), ('right', 1), ('subreddit', 1), ('keep', 1), ('getting', 1), ('better', 1)]\n",
      "---shitpost---\n",
      "[('like', 93), ('mcgill', 88), ('people', 82), ('student', 75), ('get', 71), ('class', 65), ('one', 57), ('think', 56), ('know', 54), ('deleted', 53)]\n",
      "---Political---\n",
      "[('people', 281), ('mcgill', 209), ('protest', 177), ('think', 158), ('like', 141), ('student', 129), ('israel', 125), ('encampment', 119), ('one', 113), ('palestinian', 110)]\n",
      "---I should be working rn---\n",
      "[('posture', 4), ('like', 3), ('bad', 3), ('feel', 2), ('attacked', 2), ('smh', 2), ('back', 2), ('check', 1), ('pic', 1), ('picture', 1)]\n",
      "---MEGATHREAD---\n",
      "[('ssmu', 143), ('student', 122), ('people', 110), ('mcgill', 93), ('israel', 93), ('palestinian', 90), ('exam', 87), ('like', 78), ('right', 72), ('think', 65)]\n",
      "---University Politics---\n",
      "[('student', 82), ('ssmu', 57), ('benefit', 42), ('bar', 41), ('like', 41), ('people', 41), ('gerts', 35), ('mcgill', 32), ('right', 29), ('think', 29)]\n",
      "---TW: sexual violence---\n",
      "[('mcgill', 30), ('people', 20), ('sexual', 17), ('assault', 15), ('campus', 15), ('victim', 15), ('think', 13), ('student', 13), ('like', 12), ('really', 12)]\n",
      "---From Laurier University---\n",
      "[('student', 12), ('prof', 11), ('like', 9), ('cheat', 9), ('class', 7), ('cheating', 6), ('report', 5), ('people', 5), ('pretty', 5), ('mcgill', 5)]\n",
      "---TW: Suicide---\n",
      "[('mcgill', 12), ('need', 11), ('mental', 10), ('health', 10), ('help', 10), ('feel', 10), ('like', 10), ('time', 9), ('please', 8), ('reach', 8)]\n",
      "---Martlet Club---\n",
      "[('people', 46), ('virus', 33), ('chinese', 25), ('comment', 25), ('deleted', 19), ('post', 14), ('say', 14), ('china', 13), ('speech', 12), ('bad', 12)]\n",
      "---coronavirus---\n",
      "[('covid', 13), ('people', 11), ('mcgill', 10), ('know', 9), ('number', 7), ('test', 7), ('get', 6), ('think', 6), ('like', 6), ('immunity', 6)]\n",
      "---HQ Shitpost---\n",
      "[('prof', 9), ('lecture', 7), ('video', 6), ('like', 6), ('really', 5), ('one', 5), ('experience', 4), ('people', 4), ('kinda', 4), ('started', 4)]\n",
      "---Wholesome Post---\n",
      "[('cloudberry', 6), ('boy', 4), ('cute', 3), ('look', 3), ('like', 2), ('squirrel', 2), ('best', 2), ('lesser', 1), ('known', 1), ('brother', 1)]\n",
      "---artsy shitpost---\n",
      "[('mcgill', 4), ('polisci', 3), ('see', 3), ('chaotic', 2), ('like', 2), ('concordia', 2), ('look', 2), ('people', 2), ('work', 2), ('btw', 2)]\n",
      "---SSMU---\n",
      "[('ssmu', 11), ('email', 10), ('holiday', 8), ('remembrance', 6), ('day', 6), ('country', 6), ('sent', 5), ('like', 5), ('even', 5), ('mcgill', 5)]\n",
      "---megathread---\n",
      "[('people', 28), ('online', 24), ('get', 22), ('day', 20), ('measure', 18), ('curfew', 18), ('school', 17), ('like', 17), ('covid', 16), ('think', 16)]\n",
      "---Resolved! ---\n",
      "[('da', 16), ('back', 7), ('red', 6), ('reddit', 6), ('blue', 4), ('guess', 4), ('who', 4), ('please', 3), ('use', 3), ('white', 3)]\n",
      "---TW: sexual harrassment---\n",
      "[('feel', 18), ('get', 13), ('mcgill', 10), ('help', 10), ('like', 10), ('report', 9), ('sorry', 9), ('people', 9), ('apartment', 8), ('know', 8)]\n",
      "---Stay Home---\n",
      "[('mcgill', 105), ('student', 100), ('class', 94), ('online', 85), ('know', 85), ('http', 83), ('week', 74), ('like', 67), ('time', 60), ('going', 60)]\n",
      "---Funny/Meme---\n",
      "[('think', 13), ('something', 12), ('people', 11), ('said', 9), ('class', 8), ('like', 8), ('girl', 8), ('american', 8), ('comment', 8), ('person', 8)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_topk_words(df, k):\n",
    "    '''\n",
    "    Takes a df and returns top k words from tokens\n",
    "    '''\n",
    "    all_tokens = [token for row in df['lemmatized_tokens'] for token in row]\n",
    "\n",
    "    # Count frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Get top K words\n",
    "    top_k = token_counts.most_common(k)\n",
    "\n",
    "    return top_k\n",
    "\n",
    "# Get top 10 most common words across all posts\n",
    "print(f'---All Posts---\\n{get_topk_words(df, 10)}')\n",
    "\n",
    "# Get top 10 most common words across flairs\n",
    "for flair in df['flair'].unique():\n",
    "    print(f'---{flair}---\\n{get_topk_words(df[df['flair'] == flair], 10)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b3da5",
   "metadata": {},
   "source": [
    "It seems all posts feature words that are probably common throughout the entire subreddit. Need to figure out a way to explore specific topics people are talking about.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f45da",
   "metadata": {},
   "source": [
    "Mutual Information (MI). If we treat our flair as a rough topic label, we can use MI to tell us information that is shared in the non-linear relationships between tokens and flairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28269221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n",
      "/var/folders/h1/w_jtl0654ns96y8ngd075fj40000gn/T/ipykernel_12971/3938661492.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))\n"
     ]
    }
   ],
   "source": [
    "# 500 most common words\n",
    "top_500 = get_topk_words(df, 500)\n",
    "\n",
    "# Create matrix where columns have bool indicating whether specific post has a word \n",
    "words_df = pd.DataFrame(df['flair'])\n",
    "\n",
    "for word, _ in top_500:\n",
    "    words_df[word] = df['lemmatized_tokens'].apply(lambda tokens: int(word in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61f7cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def getMI(topk, df, label_column = 'flair'):\n",
    "\n",
    "    # Get flair names\n",
    "    unique_labels = df[label_column].unique()\n",
    "\n",
    "    # Placeholder df\n",
    "    overalldf = pd.DataFrame()\n",
    "\n",
    "    for flair in unique_labels:\n",
    "        miscore = []\n",
    "        label = df[label_column].copy()\n",
    "\n",
    "\n",
    "        label[label != flair] = 0\n",
    "        label[label == flair] = 1\n",
    "\n",
    "        # Get MI for topk words in flair\n",
    "        for word in topk:\n",
    "            miscore.append([word[0]] + [mutual_info_score(label, df[word[0]])] + [flair])\n",
    "\n",
    "        # Combine scores of all words for flair into df\n",
    "        miscoredf = pd.DataFrame(miscore).sort_values(1, ascending = False)\n",
    "        miscoredf.columns = ['word', 'mi', 'flair']\n",
    "        overalldf = pd.concat([overalldf, miscoredf])\n",
    "\n",
    "    return overalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7c39a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>word</th>\n",
       "      <th>mi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100% super duper confirmed by the r/byssey</td>\n",
       "      <td>vaccine</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100% super duper confirmed by the r/byssey</td>\n",
       "      <td>french</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100% super duper confirmed by the r/byssey</td>\n",
       "      <td>staff</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100% super duper confirmed by the r/byssey</td>\n",
       "      <td>f</td>\n",
       "      <td>0.000806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100% super duper confirmed by the r/byssey</td>\n",
       "      <td>apply</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>üî•üî•üî•</td>\n",
       "      <td>member</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>üî•üî•üî•</td>\n",
       "      <td>teaching</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>üî•üî•üî•</td>\n",
       "      <td>society</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>üî•üî•üî•</td>\n",
       "      <td>company</td>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>üî•üî•üî•</td>\n",
       "      <td>business</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          flair      word        mi\n",
       "0    100% super duper confirmed by the r/byssey   vaccine  0.001143\n",
       "1    100% super duper confirmed by the r/byssey    french  0.001017\n",
       "2    100% super duper confirmed by the r/byssey     staff  0.000835\n",
       "3    100% super duper confirmed by the r/byssey         f  0.000806\n",
       "4    100% super duper confirmed by the r/byssey     apply  0.000770\n",
       "..                                          ...       ...       ...\n",
       "745                                         üî•üî•üî•    member  0.000820\n",
       "746                                         üî•üî•üî•  teaching  0.000819\n",
       "747                                         üî•üî•üî•   society  0.000815\n",
       "748                                         üî•üî•üî•   company  0.000811\n",
       "749                                         üî•üî•üî•  business  0.000793\n",
       "\n",
       "[750 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_scores = getMI(top_500, words_df)\n",
    "mi_scores.set_index(['word']).groupby('flair')['mi'].nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4412e3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bedba697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def evalsentences(sentences, to_df = False, columns = []):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pdlist = []\n",
    "\n",
    "    if to_df:\n",
    "        for sentence in sentences:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence] + [ss['compound']])\n",
    "        df = pd.DataFrame(pdlist)\n",
    "        df.columns = columns\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            print('\\n' + sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            for k in sorted(ss):\n",
    "                print('{0}: {1}, '.format(k, ss[k], end = ''))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4892a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df['combined_text']\n",
    "vader_df = evalsentences(comments, to_df = True, columns = ['comments', 'vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903c69ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ik this man, he‚Äôs getting cooked. maybe uoft won‚Äôt do anything but it‚Äôs over for him and his social life esp w his gf\\xa0', \"he apparently deleted his twitter account too, it's geniuenly over for bro\", 'i think the worst part of this has to be the fact that he peaked top 500 na in valorant.', \"'christ is king' jesus would def not approve of what you're saying buddy üò≠\", 'pretty deranged of him to say that you and he probably got that from nick fuentes who is a self described nazi. report him for sure.', 'saw this posted on twitter and a bunch of girls came out of the woodwork w their own screenshots too', 'has anyone ever been suspended from uoft for something like this?', '[deleted]', 'as a montreal canadiens fan, we do not claim him. he‚Äôs on his own smh. \\n\\nwhat an ass.', 'some people really don‚Äôt know how to be a normal human being', 'average utsg cs experience', 'anyone got his linkedin ? üòÇ', \"commenting to let you know that i'm not even a uoft student, and the algorithm just...</td>\n",
       "      <td>-0.9978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uoft faculty association votes to divest from israel  ['big w, honestly im surprised', 'w faculty', 'honestly amazing to hear!', 'proud of my alma mater!', 'finally', 'noone in the planet will be free until palestine is free.\\n\\nacademics (people with great and undeniable intelligence) understand that.\\n\\nüëè üëè üëè', \"finally. now let's see them actually do it\", 'it took them long enough bro', 'about time', 'well done, uoft! i‚Äôm really surprised, to be honest. but good on you!', 'i think both sides of the conflict are assholes for asking anyone to advocate for their beliefs that lead to murder. \\n\\nbut equally appalling is giving either side $ to promote their cultures.', 'this is misleading. they voted to recommend to the board that they vote to divest. plus it was barely over 50% of a vote that only like 4% of faculty engaged is. this affects and means nothing', 'wait we‚Äôre not funding genocide. ??', 'absurd how a canadian university has to take a vote in order to stop funding a fore...</td>\n",
       "      <td>-0.9594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uoft academic offence cases are literally so entertaining  ['\"the panel does not accept this explanation as it defies common sense and logic\"\\n\\nmeme material', 'damn, which case is this i want to read it', 'imagine being expelled for cheating on a test worth 16%...', 'something more interesting about that case is the fact that when he went to retrieve the devices later that day, he brought someone (apparently his roommate) with him. the professor ended up returning the devices because she felt scared for her safety as she was alone at night.', 'they are soo entertaining i love reading them cause sometimes it‚Äôs shocking how much effort and money people would put into cheating instead of actually studying. i think the worst case i‚Äôve read  was probably the one where a student hired someone to personate him and then he assaulted the ta or something. also the one where a ta was paid over 1k to help students cheat. and the one where a student hacked and changed his grades üò≠', \"after th...</td>\n",
       "      <td>-0.9184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>just graduated at convocation with encampment present  just graduated and guess what, the encampment had little to no effect on the ceremony. cope mf‚Äôs who think the encampment was some huge negative for convocation. students were able to go on stage with keffiyeh and some went up with the palestinian flag and some with banners in support of the movement, and those students with the flag or banners probably got the loudest applause. overall great experience and no interruptions. \\n\\ncongrats to all my fellow grads!!! [\"that's so good to hear! i'm a huge supporter of the encampment, but of course wouldn't want anyone's graduation to get severely impacted. i'm glad we can do both!\", 'echo chamber convocation.', '[deleted]', 'congratulations on graduating. now go be productive', \"a shame that there weren't lots of israeli flags represented on stage\", '[removed]', '[deleted]', 'gross', '[removed]', 'i‚Äôm so happy i declined my offer to that school üíÄ', 'technically the cultural and flag ...</td>\n",
       "      <td>-0.9983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is this area safe? i‚Äôm new to earth. (01001101 01100001 01100100 01100101 01010101 01001100 01101111 01101111 01101011) hi all, i hope uoft has been a delightful experience so far. as a new applicant, i was curious as to whether it would be safe to reside in the area highlighted above. while i have already selected my area of study (yiddish lang) and my preferred place of residence (benches in rotman‚Äôs), over the last few weeks i‚Äôve heard some disturbing things about this area. is it safe? should i still apply? ['not safe for your wallet lol', 'you‚Äôll spontaneously combust the moment you enter circle', 'actually, the vast majority of it is not safe unless you have the technical knowledge of surviving in dense wilderness.', \"you'll get mauled by a rogue species known as the timbit. beware of his gaze, take a bite out of him if he comes near you. but if it's the sprinkled variant, just run, he tastes like shit anyways.\", 'not at all. we have polar bears wandering around. no fence bet...</td>\n",
       "      <td>-0.9680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>the auto mods on this sub are trash why the f*ck can‚Äôt i ask any question without being told it needs to go in a stupid sub thread that no one even looks at or replies to. like one keyword and it gets modded out. garbage ['fuck them mods', 'revolution ‚úä', 'preach', '[removed]', 'for real, no one cares if the post is not in the thread, plus it‚Äôs easier for people to find in the future', 'pov: [removed]', 'yeah fr', 'üò≠üò≠üò≠üò≠midterms and my deferred final got me fucked up im so sorry']</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>saw this on twitter, thought it was funny  [\" if you're a first or second year  (u0 or u1) a sizeable amount of your classes will probably be online. the whole thing about going into the bathroom to socialize is straight up nonsensical though.\", 'i mean yeah most of my classes are online, but i can still socialize outdoors without a mask and we can still hear eachother with masks on lmaooooo  \\n\\n\\nor sitting down in gerts, where you dont need a mask', 'do people realise they can socialise with masks on?', \"where's the joke?\", 'umm please wear your mask in bathrooms??? that‚Äôs gross why you tryna smell some shit and pee???? just go outside lmao', 'there are also things called cafeterias...', 'not from mcgill. can someone explain how courses are still virtual but students share the same bathroom? do you like.. have virtual teachers while you are physically in the classroom?', '[deleted]', 'my daughter‚Äôs classes are all online.  especially annoying since i seem to recall mcgill announ...</td>\n",
       "      <td>-0.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>new mcgill merch idea s/uze.\\n\\n\\nin big letters. on a hoodie. ['4am quarantine thoughts', 'this. is. genius', \"you've heard of big suze.  now get ready for: men's xl suze\", 'r/mcgill merch? u/catanoverlord', 'or on a mask!', 'i...want this so bad', '/u/arweavethis', '[deleted]', '**saved to the permaweb! [https://arweave.net/tw2ci_ncgpwtjhd-ioazrkeilrxr3anfnv5gbufr9cs](https://arweave.net/tw2ci_ncgpwtjhd-ioazrkeilrxr3anfnv5gbufr9cs)**\\n  \\n*arweavethis is a bot that permanently stores posts and comment threads on an immutable ledger, combating censorship and [the memory hole](https://en.wikipedia.org/wiki/memory_hole).*', \"i'd buy it.\", 'i feel attacked rn']</td>\n",
       "      <td>-0.2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>on sphr and the impeachment ‚Äãi attended last week‚Äôs general assembly, whose attendance was visibly segregated by a majority of sphr supporters and a minority of non-affiliated people. watching the majority of members talk and ask staged questions provided really good insight about sphr, what they want, and their modus operandi.\\n\\nit is clear, from what i saw at the assembly, that sphr is angered at ssmu for not holding a referendum to vote on a student-wide strike in support of palestine which would have occurred in november - a result of poor organization and the lack of understanding of student government. in retaliation, they are trying to impeach the ssmu president as a scapegoat.\\n\\nsphr aren't the only ones to blame for the lack of action in support of palestine - the ssmu president does have some responsibility for this, but has apologized during the meeting. however, as a result of the [ongoing provincial injunction](https://ssmu.ca/blog/2024/08/statement-regarding-legal-u...</td>\n",
       "      <td>-0.9991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>mcgill‚Äôs ssmu is the worst most divisive student union. instead of fostering unity, they meddle in politics overseas that insults or harms their very own students that they were elected to represent. \\n\\nmore mcgill students need to vote and be very cautious when electing our reps. \\n\\ni have zero trust in them, and clearly our justice system doesn‚Äôt either. [\"this post has been reported for brigading. right now, it will stay up because it is not *technically* brigading. \\n\\nop does misrepresent the fact that they are no longer a mcgill student, and they do so in a rather dishonest way. that does deserve criticism (and they are getting plenty in the comments, most of it deservedly so).\\n\\nhowever, they have been posting on /r/mcgill for several years, including older comments that address specific montreal and mcgill issues, which pushes me to believe they *used* to be a mcgill student, and they are a relatively regular user of this subreddit. they also do not directly claim to be ...</td>\n",
       "      <td>-0.9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comments  \\\n",
       "1     ['ik this man, he‚Äôs getting cooked. maybe uoft won‚Äôt do anything but it‚Äôs over for him and his social life esp w his gf\\xa0', \"he apparently deleted his twitter account too, it's geniuenly over for bro\", 'i think the worst part of this has to be the fact that he peaked top 500 na in valorant.', \"'christ is king' jesus would def not approve of what you're saying buddy üò≠\", 'pretty deranged of him to say that you and he probably got that from nick fuentes who is a self described nazi. report him for sure.', 'saw this posted on twitter and a bunch of girls came out of the woodwork w their own screenshots too', 'has anyone ever been suspended from uoft for something like this?', '[deleted]', 'as a montreal canadiens fan, we do not claim him. he‚Äôs on his own smh. \\n\\nwhat an ass.', 'some people really don‚Äôt know how to be a normal human being', 'average utsg cs experience', 'anyone got his linkedin ? üòÇ', \"commenting to let you know that i'm not even a uoft student, and the algorithm just...   \n",
       "4     uoft faculty association votes to divest from israel  ['big w, honestly im surprised', 'w faculty', 'honestly amazing to hear!', 'proud of my alma mater!', 'finally', 'noone in the planet will be free until palestine is free.\\n\\nacademics (people with great and undeniable intelligence) understand that.\\n\\nüëè üëè üëè', \"finally. now let's see them actually do it\", 'it took them long enough bro', 'about time', 'well done, uoft! i‚Äôm really surprised, to be honest. but good on you!', 'i think both sides of the conflict are assholes for asking anyone to advocate for their beliefs that lead to murder. \\n\\nbut equally appalling is giving either side $ to promote their cultures.', 'this is misleading. they voted to recommend to the board that they vote to divest. plus it was barely over 50% of a vote that only like 4% of faculty engaged is. this affects and means nothing', 'wait we‚Äôre not funding genocide. ??', 'absurd how a canadian university has to take a vote in order to stop funding a fore...   \n",
       "5     uoft academic offence cases are literally so entertaining  ['\"the panel does not accept this explanation as it defies common sense and logic\"\\n\\nmeme material', 'damn, which case is this i want to read it', 'imagine being expelled for cheating on a test worth 16%...', 'something more interesting about that case is the fact that when he went to retrieve the devices later that day, he brought someone (apparently his roommate) with him. the professor ended up returning the devices because she felt scared for her safety as she was alone at night.', 'they are soo entertaining i love reading them cause sometimes it‚Äôs shocking how much effort and money people would put into cheating instead of actually studying. i think the worst case i‚Äôve read  was probably the one where a student hired someone to personate him and then he assaulted the ta or something. also the one where a ta was paid over 1k to help students cheat. and the one where a student hacked and changed his grades üò≠', \"after th...   \n",
       "13    just graduated at convocation with encampment present  just graduated and guess what, the encampment had little to no effect on the ceremony. cope mf‚Äôs who think the encampment was some huge negative for convocation. students were able to go on stage with keffiyeh and some went up with the palestinian flag and some with banners in support of the movement, and those students with the flag or banners probably got the loudest applause. overall great experience and no interruptions. \\n\\ncongrats to all my fellow grads!!! [\"that's so good to hear! i'm a huge supporter of the encampment, but of course wouldn't want anyone's graduation to get severely impacted. i'm glad we can do both!\", 'echo chamber convocation.', '[deleted]', 'congratulations on graduating. now go be productive', \"a shame that there weren't lots of israeli flags represented on stage\", '[removed]', '[deleted]', 'gross', '[removed]', 'i‚Äôm so happy i declined my offer to that school üíÄ', 'technically the cultural and flag ...   \n",
       "15    is this area safe? i‚Äôm new to earth. (01001101 01100001 01100100 01100101 01010101 01001100 01101111 01101111 01101011) hi all, i hope uoft has been a delightful experience so far. as a new applicant, i was curious as to whether it would be safe to reside in the area highlighted above. while i have already selected my area of study (yiddish lang) and my preferred place of residence (benches in rotman‚Äôs), over the last few weeks i‚Äôve heard some disturbing things about this area. is it safe? should i still apply? ['not safe for your wallet lol', 'you‚Äôll spontaneously combust the moment you enter circle', 'actually, the vast majority of it is not safe unless you have the technical knowledge of surviving in dense wilderness.', \"you'll get mauled by a rogue species known as the timbit. beware of his gaze, take a bite out of him if he comes near you. but if it's the sprinkled variant, just run, he tastes like shit anyways.\", 'not at all. we have polar bears wandering around. no fence bet...   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "2977                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     the auto mods on this sub are trash why the f*ck can‚Äôt i ask any question without being told it needs to go in a stupid sub thread that no one even looks at or replies to. like one keyword and it gets modded out. garbage ['fuck them mods', 'revolution ‚úä', 'preach', '[removed]', 'for real, no one cares if the post is not in the thread, plus it‚Äôs easier for people to find in the future', 'pov: [removed]', 'yeah fr', 'üò≠üò≠üò≠üò≠midterms and my deferred final got me fucked up im so sorry']   \n",
       "2980  saw this on twitter, thought it was funny  [\" if you're a first or second year  (u0 or u1) a sizeable amount of your classes will probably be online. the whole thing about going into the bathroom to socialize is straight up nonsensical though.\", 'i mean yeah most of my classes are online, but i can still socialize outdoors without a mask and we can still hear eachother with masks on lmaooooo  \\n\\n\\nor sitting down in gerts, where you dont need a mask', 'do people realise they can socialise with masks on?', \"where's the joke?\", 'umm please wear your mask in bathrooms??? that‚Äôs gross why you tryna smell some shit and pee???? just go outside lmao', 'there are also things called cafeterias...', 'not from mcgill. can someone explain how courses are still virtual but students share the same bathroom? do you like.. have virtual teachers while you are physically in the classroom?', '[deleted]', 'my daughter‚Äôs classes are all online.  especially annoying since i seem to recall mcgill announ...   \n",
       "2990                                                                                                                                                                                                                                                                                                                                              new mcgill merch idea s/uze.\\n\\n\\nin big letters. on a hoodie. ['4am quarantine thoughts', 'this. is. genius', \"you've heard of big suze.  now get ready for: men's xl suze\", 'r/mcgill merch? u/catanoverlord', 'or on a mask!', 'i...want this so bad', '/u/arweavethis', '[deleted]', '**saved to the permaweb! [https://arweave.net/tw2ci_ncgpwtjhd-ioazrkeilrxr3anfnv5gbufr9cs](https://arweave.net/tw2ci_ncgpwtjhd-ioazrkeilrxr3anfnv5gbufr9cs)**\\n  \\n*arweavethis is a bot that permanently stores posts and comment threads on an immutable ledger, combating censorship and [the memory hole](https://en.wikipedia.org/wiki/memory_hole).*', \"i'd buy it.\", 'i feel attacked rn']   \n",
       "2992  on sphr and the impeachment ‚Äãi attended last week‚Äôs general assembly, whose attendance was visibly segregated by a majority of sphr supporters and a minority of non-affiliated people. watching the majority of members talk and ask staged questions provided really good insight about sphr, what they want, and their modus operandi.\\n\\nit is clear, from what i saw at the assembly, that sphr is angered at ssmu for not holding a referendum to vote on a student-wide strike in support of palestine which would have occurred in november - a result of poor organization and the lack of understanding of student government. in retaliation, they are trying to impeach the ssmu president as a scapegoat.\\n\\nsphr aren't the only ones to blame for the lack of action in support of palestine - the ssmu president does have some responsibility for this, but has apologized during the meeting. however, as a result of the [ongoing provincial injunction](https://ssmu.ca/blog/2024/08/statement-regarding-legal-u...   \n",
       "2993  mcgill‚Äôs ssmu is the worst most divisive student union. instead of fostering unity, they meddle in politics overseas that insults or harms their very own students that they were elected to represent. \\n\\nmore mcgill students need to vote and be very cautious when electing our reps. \\n\\ni have zero trust in them, and clearly our justice system doesn‚Äôt either. [\"this post has been reported for brigading. right now, it will stay up because it is not *technically* brigading. \\n\\nop does misrepresent the fact that they are no longer a mcgill student, and they do so in a rather dishonest way. that does deserve criticism (and they are getting plenty in the comments, most of it deservedly so).\\n\\nhowever, they have been posting on /r/mcgill for several years, including older comments that address specific montreal and mcgill issues, which pushes me to believe they *used* to be a mcgill student, and they are a relatively regular user of this subreddit. they also do not directly claim to be ...   \n",
       "\n",
       "       vader  \n",
       "1    -0.9978  \n",
       "4    -0.9594  \n",
       "5    -0.9184  \n",
       "13   -0.9983  \n",
       "15   -0.9680  \n",
       "...      ...  \n",
       "2977 -0.4019  \n",
       "2980 -0.9944  \n",
       "2990 -0.2695  \n",
       "2992 -0.9991  \n",
       "2993 -0.9999  \n",
       "\n",
       "[667 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what comments are rated negative from VADER\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "vader_df[vader_df['vader'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0f5bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to the student who got caught using ai on their exam at uoft law thanks a lot you dumb f*ck. due to your idiocy and dumbassery my college (not in ontario) is now cracking down on access to our hard drives during exams, so now we have to print everything which will be a lot of time and money i don‚Äôt have. if you‚Äôre too incompetent to write a law school exam and require ai then you shouldn‚Äôt even be in law. thanks for ruining it for every other college across the country. ['[deleted]', 'it‚Äôs crazy that there‚Äôs people mad at you for venting.', 'gaining access to your hard drives feels like a huge violation of privacy? i‚Äôm surprised that‚Äôs even allowed.', '?', '[removed]', 'what canadian says \"college?\"', 'sighs, sounds exhausting. best of luck', 'oof. i only use ai for helping me understand concepts (and actually backing up what it says with sources). idk why so many ppl use it to write their papers for them. it‚Äôs not that hard to write your own ideas!', 'thanks to the idiot who got c...</td>\n",
       "      <td>0.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want a ps5  ['i love  how low effort this looks', 'pain', 'loool u made my day..', 'those boys and scalpers really took those things quick. shoppers lied to me and abandoned me while they stole every playstayion', 'lmfao üò≠\\n\\ni hate this school. i can‚Äôt wait to graduate.', 'i managed to order one. coming next monday ;)', 'i live in problem set 5 but the \"et 5\" is silent...', 'this is a quality post', 'nice', 'yoooo this was hilarious. had my eyes wide open and i was cackling', 'this is too good', 'same, but no my mom wants to get ‚Äúthe family‚Äù a nintendo switch üò≠üò≠', 'Ê≤°Êúâps5:(', 'same üò©', 'i don‚Äôt get it', 'did you get one?', '[you should find this ad on facebook marketplace. ](https://imgur.com/gallery/gkv44cr)', 'terrible', 'ummmm... so this is what uoft students do on their free time?', 'buy a computer for the same price and run the same games 10 times as fast wow am a genius who isn‚Äôt falling for sony‚Äôs shitty ass marketing because now you can play the same games better quality ...</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if this thread gets over 300 upvotes in the next 24h i will change the sub to r/idealcatering until exams are over srsly don't fuck this up for me, we're already working on the css don't want this shit to go to waste\\n\\nedit: holy mother of god we're working on asap\\n\\nedit 2: yo r/uwaterloo [i guess we just won the meme war](https://www.reddit.com/r/uoft/comments/5i7f1g/if_this_thread_gets_over_300_upvotes_in_the_next/db6b2rg/)\\n\\nedit 3: ...and now i just got gilded what is this life, should have the update applied sometime tommorow\\n\\nedit 4: [it's been done](https://www.reddit.com/r/uoft/comments/5ic84a/welcome_to_ridealcatering/) ['[deleted]', '[deleted]', \"found this in /r/all/rising, i'll give my updoot.\", \"/r/uwaterloo's top post in their sub's history got 670 upvotes. at the time of this writing, this post has gotten 757 upvotes in 4 hours...\\n\\nnot sure if i should feel proud about this.\", '**any**', '[deleted]', \" /r/all here what's ideal catering\", \"the [ass goose](http...</td>\n",
       "      <td>0.9986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i was playing chess with a girl, i ended up telling her she was so mateable as a cs major i have a hobby of chess, i finally found a girl who‚Äôll play chess with me. she plays but clearly not as much as me, won like 5 times in a row. ended saying loudly ‚Äùwhy you so mateable!‚Äù trying to trash talk her. she laughed then looked at me weird and then people beside me looked at me weird. 2 hrs later i finally realized. i am making sure i won‚Äòt be mating anytime irl.  [\"whenever stories like these pop up, the people in them insist on making it known that they're cs majors for some strange reason. \\n\\nlike, chill. we can already deduce this from the details provided.\", 'ts can‚Äôt be fr vro üíîü•Ä', 'yikes', 'finally, cs student not being straight men kissing each other', '‚Äúwhy you so matable!‚Äù *stares for 3 seconds ‚Äúcheck ‚Ä¶ mate, of course‚Äù', \"you should tell her she's breedable next time #unspokenrizz #bahenbro\", 'computer students being computer students :', 'you had 1 word to put in your sent...</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lady drives into cement in front of st george station üò≠  ['thank you for posting concrete evidence of her actions', 'i always wonder how these dumbasses end up getting past road closed signs and blockings', 'workers taking pics ‚Ä¶ üòÇ', 'dang a new car too, that license plate number is from the last few months with the dcxx üò≠', 'what in the negative iq', 'this is real üò≠üò≠üò≠?? oh my god how did she live this long!', 'rotman spotted in the wild', \"that's conk creat babey\", 'curious to know what happened after. did she try to reverse out? did the concrete dry? how did she get herself out?\\xa0\\n\\n\\nneed details.', 'people drive into streetcar tunnels sometimes lmfao', 'she must‚Äôve taken that sign for granite.', '99% she gets angry instead admitting her fault', 'oopsie', 'did she stop because she realized there‚Äôs actual concrete or because her car wouldn‚Äôt move anymore?', 'apple maps user', 'i was the driver in the cement truck, im the guy in the yellow. she drove into it a couple minutes af...</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>the library is pretty awesome my mind was blown today when i was browsing at mclennan basement and came across a shelf with the *original volumes* of encyclopedia britannica from 1879, just sitting there. how much undiscovered treasure is hidden in plain sight on those dusty shelves?\\n\\nthat's all. ['as a history major, i can confirm that the library is a great place. especially when i checked out a book that hadn‚Äôt been checked out since 1987.', \"i'm currently reading a book about all the horrors that happened at mcgill during cold war time, written by a montrealler. i sometimes go to buildings where those things happened, sit there and read the chapters. paints a totally different picture.\\n\\nnot related to library but i overshare. what can i do.\", \"agreed!! i love that they have a wide genre of books outside of academic works such as classics, children's books, ya fiction and contemporary literature.\", '[removed]', \"the expanse of old books is truly amazing, as a history student...</td>\n",
       "      <td>0.9969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>do we need to go to classroom when snows? or move classroom from a building near the mountain to a building near the city? ['\"tell me you\\'re not canadian without telling me you\\'re not canadian\"', 'the only time class is cancelled is when you see a polar bear on sherbrooke.', 'this made me laugh quite a bit, thanks for that', \"idk if this is satire or a poor soul who hasn't seen snow before, either way very amusing. to answer your question though, no, it snows way often for that. you're expected to go to classes when it snows the same way you're expected to go when it's raining.\", 'i‚Äôm dead. this poor international student omg', 'the education building used to have a rope that you could use to climb up the hill if the sidewalk was unwalkable.', \"welcome to the great white north. you'll get used to the snow quick enough.\", 'technically you dont need to go to school at all lol', \"you should get them snow boots with the grips integrated cause you gna be walking up the snowy mountains...</td>\n",
       "      <td>0.5552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>is it just me or grads and undergrads really disconnected? why are grads and undergrads so disconnected? why do i need to be a ta to be connected with undergrads?\\n\\nthe thing is, grads have a lot to offer in terms of experience to the undergrad, an experience which does not have to be confined to the lab, and vice versa. i have personally learned a lot since i joined my masters program. report writing, expectations, stress management, deliverables, work quality, and so many things other things that are not necessarily technical in nature, but are just important traits to have.\\n\\nit could be a very casual relationship, where the undergrad and grad complain to each other about problems, potentially offer solutions, or maybe just listen. i am not saying strictly technical problems and homework. it could be anything. it can also help the undergrad get a glimpse on gradlife. in turn, the grad student would feel fulfilled to have talked to someone outside their extremely small circle.\\...</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>person who was vomiting at 9am exam in fieldhouse: are you ok? that was intense. ['that was crazy!! sounded like he was dying and he had a full on brown paper bag. but also unsure why the invigilators let him sit there for so long', \"in science, if you vomit at the exam you still get the grade...\\nit's the shittiest thing.\", \"yikes! he should have been removed from his seat much more quickly than he was for his own sake and everyone else's. it could be that the invigilators helping him were new and didn't know what to do right away. wishing him well!\", 'fuck! i was in the first rows and could still hear him very loudly. i hope he‚Äôs okay!!', \"how do you even get a doctors note if you don't have a doctor? i feel like people who are sick and can hardly stand and don't have doctors are super fucked and there's nothing they can do about it.\", 'a few years ago, final exams took place at the scotia bank cinema and a student puked on the carpet of one of the rooms. that was something.', 'd...</td>\n",
       "      <td>0.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>mcgill.courses is now updated for the 2025-2026 school year hey everyone,\\n\\nwe've just finished updating [mcgill.courses](https://mcgill.courses) with all the data for the 2025-2026 school year. it took a bit since we had to change quite a bit of code to adapt to the new course catalogue, and mcgill seems to insist making things harder for us üôÉ\\n\\nas a quick reminder, [mcgill.courses](http://mcgill.courses) lets you:\\n\\n* search for courses in an ergonomic way\\n* see course information and reviews all in one place\\n* view past class averages\\n* view class schedules and grab course crns for quick registration\\n* leave your own reviews for courses\\n\\nif you hate the new course catalogue, this is for you.\\n\\n[https://mcgill.courses/](https://mcgill.courses/)\\n\\nremember to review your winter term courses, and happy course planning! ['new course catalogue is awful. lacking the ability to see courses by subject, unreal. thanks for your good work here\\n\\nmy one bit of feedback is that a...</td>\n",
       "      <td>0.9296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2299 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     comments  \\\n",
       "0     to the student who got caught using ai on their exam at uoft law thanks a lot you dumb f*ck. due to your idiocy and dumbassery my college (not in ontario) is now cracking down on access to our hard drives during exams, so now we have to print everything which will be a lot of time and money i don‚Äôt have. if you‚Äôre too incompetent to write a law school exam and require ai then you shouldn‚Äôt even be in law. thanks for ruining it for every other college across the country. ['[deleted]', 'it‚Äôs crazy that there‚Äôs people mad at you for venting.', 'gaining access to your hard drives feels like a huge violation of privacy? i‚Äôm surprised that‚Äôs even allowed.', '?', '[removed]', 'what canadian says \"college?\"', 'sighs, sounds exhausting. best of luck', 'oof. i only use ai for helping me understand concepts (and actually backing up what it says with sources). idk why so many ppl use it to write their papers for them. it‚Äôs not that hard to write your own ideas!', 'thanks to the idiot who got c...   \n",
       "2     i want a ps5  ['i love  how low effort this looks', 'pain', 'loool u made my day..', 'those boys and scalpers really took those things quick. shoppers lied to me and abandoned me while they stole every playstayion', 'lmfao üò≠\\n\\ni hate this school. i can‚Äôt wait to graduate.', 'i managed to order one. coming next monday ;)', 'i live in problem set 5 but the \"et 5\" is silent...', 'this is a quality post', 'nice', 'yoooo this was hilarious. had my eyes wide open and i was cackling', 'this is too good', 'same, but no my mom wants to get ‚Äúthe family‚Äù a nintendo switch üò≠üò≠', 'Ê≤°Êúâps5:(', 'same üò©', 'i don‚Äôt get it', 'did you get one?', '[you should find this ad on facebook marketplace. ](https://imgur.com/gallery/gkv44cr)', 'terrible', 'ummmm... so this is what uoft students do on their free time?', 'buy a computer for the same price and run the same games 10 times as fast wow am a genius who isn‚Äôt falling for sony‚Äôs shitty ass marketing because now you can play the same games better quality ...   \n",
       "3     if this thread gets over 300 upvotes in the next 24h i will change the sub to r/idealcatering until exams are over srsly don't fuck this up for me, we're already working on the css don't want this shit to go to waste\\n\\nedit: holy mother of god we're working on asap\\n\\nedit 2: yo r/uwaterloo [i guess we just won the meme war](https://www.reddit.com/r/uoft/comments/5i7f1g/if_this_thread_gets_over_300_upvotes_in_the_next/db6b2rg/)\\n\\nedit 3: ...and now i just got gilded what is this life, should have the update applied sometime tommorow\\n\\nedit 4: [it's been done](https://www.reddit.com/r/uoft/comments/5ic84a/welcome_to_ridealcatering/) ['[deleted]', '[deleted]', \"found this in /r/all/rising, i'll give my updoot.\", \"/r/uwaterloo's top post in their sub's history got 670 upvotes. at the time of this writing, this post has gotten 757 upvotes in 4 hours...\\n\\nnot sure if i should feel proud about this.\", '**any**', '[deleted]', \" /r/all here what's ideal catering\", \"the [ass goose](http...   \n",
       "6     i was playing chess with a girl, i ended up telling her she was so mateable as a cs major i have a hobby of chess, i finally found a girl who‚Äôll play chess with me. she plays but clearly not as much as me, won like 5 times in a row. ended saying loudly ‚Äùwhy you so mateable!‚Äù trying to trash talk her. she laughed then looked at me weird and then people beside me looked at me weird. 2 hrs later i finally realized. i am making sure i won‚Äòt be mating anytime irl.  [\"whenever stories like these pop up, the people in them insist on making it known that they're cs majors for some strange reason. \\n\\nlike, chill. we can already deduce this from the details provided.\", 'ts can‚Äôt be fr vro üíîü•Ä', 'yikes', 'finally, cs student not being straight men kissing each other', '‚Äúwhy you so matable!‚Äù *stares for 3 seconds ‚Äúcheck ‚Ä¶ mate, of course‚Äù', \"you should tell her she's breedable next time #unspokenrizz #bahenbro\", 'computer students being computer students :', 'you had 1 word to put in your sent...   \n",
       "7     lady drives into cement in front of st george station üò≠  ['thank you for posting concrete evidence of her actions', 'i always wonder how these dumbasses end up getting past road closed signs and blockings', 'workers taking pics ‚Ä¶ üòÇ', 'dang a new car too, that license plate number is from the last few months with the dcxx üò≠', 'what in the negative iq', 'this is real üò≠üò≠üò≠?? oh my god how did she live this long!', 'rotman spotted in the wild', \"that's conk creat babey\", 'curious to know what happened after. did she try to reverse out? did the concrete dry? how did she get herself out?\\xa0\\n\\n\\nneed details.', 'people drive into streetcar tunnels sometimes lmfao', 'she must‚Äôve taken that sign for granite.', '99% she gets angry instead admitting her fault', 'oopsie', 'did she stop because she realized there‚Äôs actual concrete or because her car wouldn‚Äôt move anymore?', 'apple maps user', 'i was the driver in the cement truck, im the guy in the yellow. she drove into it a couple minutes af...   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "2988  the library is pretty awesome my mind was blown today when i was browsing at mclennan basement and came across a shelf with the *original volumes* of encyclopedia britannica from 1879, just sitting there. how much undiscovered treasure is hidden in plain sight on those dusty shelves?\\n\\nthat's all. ['as a history major, i can confirm that the library is a great place. especially when i checked out a book that hadn‚Äôt been checked out since 1987.', \"i'm currently reading a book about all the horrors that happened at mcgill during cold war time, written by a montrealler. i sometimes go to buildings where those things happened, sit there and read the chapters. paints a totally different picture.\\n\\nnot related to library but i overshare. what can i do.\", \"agreed!! i love that they have a wide genre of books outside of academic works such as classics, children's books, ya fiction and contemporary literature.\", '[removed]', \"the expanse of old books is truly amazing, as a history student...   \n",
       "2989  do we need to go to classroom when snows? or move classroom from a building near the mountain to a building near the city? ['\"tell me you\\'re not canadian without telling me you\\'re not canadian\"', 'the only time class is cancelled is when you see a polar bear on sherbrooke.', 'this made me laugh quite a bit, thanks for that', \"idk if this is satire or a poor soul who hasn't seen snow before, either way very amusing. to answer your question though, no, it snows way often for that. you're expected to go to classes when it snows the same way you're expected to go when it's raining.\", 'i‚Äôm dead. this poor international student omg', 'the education building used to have a rope that you could use to climb up the hill if the sidewalk was unwalkable.', \"welcome to the great white north. you'll get used to the snow quick enough.\", 'technically you dont need to go to school at all lol', \"you should get them snow boots with the grips integrated cause you gna be walking up the snowy mountains...   \n",
       "2991  is it just me or grads and undergrads really disconnected? why are grads and undergrads so disconnected? why do i need to be a ta to be connected with undergrads?\\n\\nthe thing is, grads have a lot to offer in terms of experience to the undergrad, an experience which does not have to be confined to the lab, and vice versa. i have personally learned a lot since i joined my masters program. report writing, expectations, stress management, deliverables, work quality, and so many things other things that are not necessarily technical in nature, but are just important traits to have.\\n\\nit could be a very casual relationship, where the undergrad and grad complain to each other about problems, potentially offer solutions, or maybe just listen. i am not saying strictly technical problems and homework. it could be anything. it can also help the undergrad get a glimpse on gradlife. in turn, the grad student would feel fulfilled to have talked to someone outside their extremely small circle.\\...   \n",
       "2994  person who was vomiting at 9am exam in fieldhouse: are you ok? that was intense. ['that was crazy!! sounded like he was dying and he had a full on brown paper bag. but also unsure why the invigilators let him sit there for so long', \"in science, if you vomit at the exam you still get the grade...\\nit's the shittiest thing.\", \"yikes! he should have been removed from his seat much more quickly than he was for his own sake and everyone else's. it could be that the invigilators helping him were new and didn't know what to do right away. wishing him well!\", 'fuck! i was in the first rows and could still hear him very loudly. i hope he‚Äôs okay!!', \"how do you even get a doctors note if you don't have a doctor? i feel like people who are sick and can hardly stand and don't have doctors are super fucked and there's nothing they can do about it.\", 'a few years ago, final exams took place at the scotia bank cinema and a student puked on the carpet of one of the rooms. that was something.', 'd...   \n",
       "2995  mcgill.courses is now updated for the 2025-2026 school year hey everyone,\\n\\nwe've just finished updating [mcgill.courses](https://mcgill.courses) with all the data for the 2025-2026 school year. it took a bit since we had to change quite a bit of code to adapt to the new course catalogue, and mcgill seems to insist making things harder for us üôÉ\\n\\nas a quick reminder, [mcgill.courses](http://mcgill.courses) lets you:\\n\\n* search for courses in an ergonomic way\\n* see course information and reviews all in one place\\n* view past class averages\\n* view class schedules and grab course crns for quick registration\\n* leave your own reviews for courses\\n\\nif you hate the new course catalogue, this is for you.\\n\\n[https://mcgill.courses/](https://mcgill.courses/)\\n\\nremember to review your winter term courses, and happy course planning! ['new course catalogue is awful. lacking the ability to see courses by subject, unreal. thanks for your good work here\\n\\nmy one bit of feedback is that a...   \n",
       "\n",
       "       vader  \n",
       "0     0.9993  \n",
       "2     0.9980  \n",
       "3     0.9986  \n",
       "6     0.9990  \n",
       "7     0.9985  \n",
       "...      ...  \n",
       "2988  0.9969  \n",
       "2989  0.5552  \n",
       "2991  0.9994  \n",
       "2994  0.9750  \n",
       "2995  0.9296  \n",
       "\n",
       "[2299 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what comments are rated negative from VADER\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "vader_df[vader_df['vader'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcd58538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>vader</th>\n",
       "      <th>flair</th>\n",
       "      <th>people</th>\n",
       "      <th>like</th>\n",
       "      <th>student</th>\n",
       "      <th>get</th>\n",
       "      <th>year</th>\n",
       "      <th>one</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>seriously</th>\n",
       "      <th>tried</th>\n",
       "      <th>teacher</th>\n",
       "      <th>war</th>\n",
       "      <th>bring</th>\n",
       "      <th>happens</th>\n",
       "      <th>towards</th>\n",
       "      <th>office</th>\n",
       "      <th>easier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to the student who got caught using ai on their exam at uoft law thanks a lot you dumb f*ck. due to your idiocy and dumbassery my college (not in ontario) is now cracking down on access to our hard drives during exams, so now we have to print everything which will be a lot of time and money i don‚Äôt have. if you‚Äôre too incompetent to write a law school exam and require ai then you shouldn‚Äôt even be in law. thanks for ruining it for every other college across the country. ['[deleted]', 'it‚Äôs crazy that there‚Äôs people mad at you for venting.', 'gaining access to your hard drives feels like a huge violation of privacy? i‚Äôm surprised that‚Äôs even allowed.', '?', '[removed]', 'what canadian says \"college?\"', 'sighs, sounds exhausting. best of luck', 'oof. i only use ai for helping me understand concepts (and actually backing up what it says with sources). idk why so many ppl use it to write their papers for them. it‚Äôs not that hard to write your own ideas!', 'thanks to the idiot who got c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>Transfers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ik this man, he‚Äôs getting cooked. maybe uoft won‚Äôt do anything but it‚Äôs over for him and his social life esp w his gf\\xa0', \"he apparently deleted his twitter account too, it's geniuenly over for bro\", 'i think the worst part of this has to be the fact that he peaked top 500 na in valorant.', \"'christ is king' jesus would def not approve of what you're saying buddy üò≠\", 'pretty deranged of him to say that you and he probably got that from nick fuentes who is a self described nazi. report him for sure.', 'saw this posted on twitter and a bunch of girls came out of the woodwork w their own screenshots too', 'has anyone ever been suspended from uoft for something like this?', '[deleted]', 'as a montreal canadiens fan, we do not claim him. he‚Äôs on his own smh. \\n\\nwhat an ass.', 'some people really don‚Äôt know how to be a normal human being', 'average utsg cs experience', 'anyone got his linkedin ? üòÇ', \"commenting to let you know that i'm not even a uoft student, and the algorithm just...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.9978</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want a ps5  ['i love  how low effort this looks', 'pain', 'loool u made my day..', 'those boys and scalpers really took those things quick. shoppers lied to me and abandoned me while they stole every playstayion', 'lmfao üò≠\\n\\ni hate this school. i can‚Äôt wait to graduate.', 'i managed to order one. coming next monday ;)', 'i live in problem set 5 but the \"et 5\" is silent...', 'this is a quality post', 'nice', 'yoooo this was hilarious. had my eyes wide open and i was cackling', 'this is too good', 'same, but no my mom wants to get ‚Äúthe family‚Äù a nintendo switch üò≠üò≠', 'Ê≤°Êúâps5:(', 'same üò©', 'i don‚Äôt get it', 'did you get one?', '[you should find this ad on facebook marketplace. ](https://imgur.com/gallery/gkv44cr)', 'terrible', 'ummmm... so this is what uoft students do on their free time?', 'buy a computer for the same price and run the same games 10 times as fast wow am a genius who isn‚Äôt falling for sony‚Äôs shitty ass marketing because now you can play the same games better quality ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>Humour</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if this thread gets over 300 upvotes in the next 24h i will change the sub to r/idealcatering until exams are over srsly don't fuck this up for me, we're already working on the css don't want this shit to go to waste\\n\\nedit: holy mother of god we're working on asap\\n\\nedit 2: yo r/uwaterloo [i guess we just won the meme war](https://www.reddit.com/r/uoft/comments/5i7f1g/if_this_thread_gets_over_300_upvotes_in_the_next/db6b2rg/)\\n\\nedit 3: ...and now i just got gilded what is this life, should have the update applied sometime tommorow\\n\\nedit 4: [it's been done](https://www.reddit.com/r/uoft/comments/5ic84a/welcome_to_ridealcatering/) ['[deleted]', '[deleted]', \"found this in /r/all/rising, i'll give my updoot.\", \"/r/uwaterloo's top post in their sub's history got 670 upvotes. at the time of this writing, this post has gotten 757 upvotes in 4 hours...\\n\\nnot sure if i should feel proud about this.\", '**any**', '[deleted]', \" /r/all here what's ideal catering\", \"the [ass goose](http...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>Serious</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uoft faculty association votes to divest from israel  ['big w, honestly im surprised', 'w faculty', 'honestly amazing to hear!', 'proud of my alma mater!', 'finally', 'noone in the planet will be free until palestine is free.\\n\\nacademics (people with great and undeniable intelligence) understand that.\\n\\nüëè üëè üëè', \"finally. now let's see them actually do it\", 'it took them long enough bro', 'about time', 'well done, uoft! i‚Äôm really surprised, to be honest. but good on you!', 'i think both sides of the conflict are assholes for asking anyone to advocate for their beliefs that lead to murder. \\n\\nbut equally appalling is giving either side $ to promote their cultures.', 'this is misleading. they voted to recommend to the board that they vote to divest. plus it was barely over 50% of a vote that only like 4% of faculty engaged is. this affects and means nothing', 'wait we‚Äôre not funding genocide. ??', 'absurd how a canadian university has to take a vote in order to stop funding a fore...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.9594</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments  \\\n",
       "0  to the student who got caught using ai on their exam at uoft law thanks a lot you dumb f*ck. due to your idiocy and dumbassery my college (not in ontario) is now cracking down on access to our hard drives during exams, so now we have to print everything which will be a lot of time and money i don‚Äôt have. if you‚Äôre too incompetent to write a law school exam and require ai then you shouldn‚Äôt even be in law. thanks for ruining it for every other college across the country. ['[deleted]', 'it‚Äôs crazy that there‚Äôs people mad at you for venting.', 'gaining access to your hard drives feels like a huge violation of privacy? i‚Äôm surprised that‚Äôs even allowed.', '?', '[removed]', 'what canadian says \"college?\"', 'sighs, sounds exhausting. best of luck', 'oof. i only use ai for helping me understand concepts (and actually backing up what it says with sources). idk why so many ppl use it to write their papers for them. it‚Äôs not that hard to write your own ideas!', 'thanks to the idiot who got c...   \n",
       "1  ['ik this man, he‚Äôs getting cooked. maybe uoft won‚Äôt do anything but it‚Äôs over for him and his social life esp w his gf\\xa0', \"he apparently deleted his twitter account too, it's geniuenly over for bro\", 'i think the worst part of this has to be the fact that he peaked top 500 na in valorant.', \"'christ is king' jesus would def not approve of what you're saying buddy üò≠\", 'pretty deranged of him to say that you and he probably got that from nick fuentes who is a self described nazi. report him for sure.', 'saw this posted on twitter and a bunch of girls came out of the woodwork w their own screenshots too', 'has anyone ever been suspended from uoft for something like this?', '[deleted]', 'as a montreal canadiens fan, we do not claim him. he‚Äôs on his own smh. \\n\\nwhat an ass.', 'some people really don‚Äôt know how to be a normal human being', 'average utsg cs experience', 'anyone got his linkedin ? üòÇ', \"commenting to let you know that i'm not even a uoft student, and the algorithm just...   \n",
       "2  i want a ps5  ['i love  how low effort this looks', 'pain', 'loool u made my day..', 'those boys and scalpers really took those things quick. shoppers lied to me and abandoned me while they stole every playstayion', 'lmfao üò≠\\n\\ni hate this school. i can‚Äôt wait to graduate.', 'i managed to order one. coming next monday ;)', 'i live in problem set 5 but the \"et 5\" is silent...', 'this is a quality post', 'nice', 'yoooo this was hilarious. had my eyes wide open and i was cackling', 'this is too good', 'same, but no my mom wants to get ‚Äúthe family‚Äù a nintendo switch üò≠üò≠', 'Ê≤°Êúâps5:(', 'same üò©', 'i don‚Äôt get it', 'did you get one?', '[you should find this ad on facebook marketplace. ](https://imgur.com/gallery/gkv44cr)', 'terrible', 'ummmm... so this is what uoft students do on their free time?', 'buy a computer for the same price and run the same games 10 times as fast wow am a genius who isn‚Äôt falling for sony‚Äôs shitty ass marketing because now you can play the same games better quality ...   \n",
       "3  if this thread gets over 300 upvotes in the next 24h i will change the sub to r/idealcatering until exams are over srsly don't fuck this up for me, we're already working on the css don't want this shit to go to waste\\n\\nedit: holy mother of god we're working on asap\\n\\nedit 2: yo r/uwaterloo [i guess we just won the meme war](https://www.reddit.com/r/uoft/comments/5i7f1g/if_this_thread_gets_over_300_upvotes_in_the_next/db6b2rg/)\\n\\nedit 3: ...and now i just got gilded what is this life, should have the update applied sometime tommorow\\n\\nedit 4: [it's been done](https://www.reddit.com/r/uoft/comments/5ic84a/welcome_to_ridealcatering/) ['[deleted]', '[deleted]', \"found this in /r/all/rising, i'll give my updoot.\", \"/r/uwaterloo's top post in their sub's history got 670 upvotes. at the time of this writing, this post has gotten 757 upvotes in 4 hours...\\n\\nnot sure if i should feel proud about this.\", '**any**', '[deleted]', \" /r/all here what's ideal catering\", \"the [ass goose](http...   \n",
       "4  uoft faculty association votes to divest from israel  ['big w, honestly im surprised', 'w faculty', 'honestly amazing to hear!', 'proud of my alma mater!', 'finally', 'noone in the planet will be free until palestine is free.\\n\\nacademics (people with great and undeniable intelligence) understand that.\\n\\nüëè üëè üëè', \"finally. now let's see them actually do it\", 'it took them long enough bro', 'about time', 'well done, uoft! i‚Äôm really surprised, to be honest. but good on you!', 'i think both sides of the conflict are assholes for asking anyone to advocate for their beliefs that lead to murder. \\n\\nbut equally appalling is giving either side $ to promote their cultures.', 'this is misleading. they voted to recommend to the board that they vote to divest. plus it was barely over 50% of a vote that only like 4% of faculty engaged is. this affects and means nothing', 'wait we‚Äôre not funding genocide. ??', 'absurd how a canadian university has to take a vote in order to stop funding a fore...   \n",
       "\n",
       "  ground_truth   vader       flair  people  like  student  get  year  one  \\\n",
       "0     positive  0.9993   Transfers       1     1        1    1     1    1   \n",
       "1     negative -0.9978  Discussion       1     1        1    1     1    1   \n",
       "2     positive  0.9980      Humour       1     1        1    1     1    1   \n",
       "3     positive  0.9986     Serious       0     1        1    1     1    1   \n",
       "4     negative -0.9594        News       1     1        1    1     0    0   \n",
       "\n",
       "   ...  news  seriously  tried  teacher  war  bring  happens  towards  office  \\\n",
       "0  ...     0          0      0        0    0      1        1        0       1   \n",
       "1  ...     1          1      1        1    1      1        0        0       1   \n",
       "2  ...     0          0      0        0    0      0        0        0       0   \n",
       "3  ...     0          0      0        0    1      0        0        0       0   \n",
       "4  ...     1          1      0        0    1      0        0        0       0   \n",
       "\n",
       "   easier  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 504 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([vader_df, words_df], axis=1)\n",
    "combined_df['ground_truth'] = combined_df['vader'].apply(lambda x: 'positive' if x > 0 else 'negative')\n",
    "cols = combined_df.columns.to_list()\n",
    "cols.insert(1, cols.pop(cols.index('ground_truth')))\n",
    "combined_df = combined_df[cols]\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebf3ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmiForAllCal(df, topk_word, gt_sentiment, label_column='ground_truth'):\n",
    "    #Try calculate all the pmi for top k and store them into one pmidf dataframe\n",
    "\n",
    "    index = [x[0] for x in topk_word]\n",
    "    pmiDf = pd.DataFrame(index=index, columns=['pmi'])\n",
    "\n",
    "    for (word, count) in tqdm(topk_word):\n",
    "        pmiDf.at[word, 'pmi'] = pmiCalc(df,word,gt_sentiment,label_column)\n",
    "\n",
    "    return pmiDf\n",
    "\n",
    "\n",
    "def pmiCalc(df, word, gt_sentiment, label_column='ground_truth'):\n",
    "\n",
    "    N = df.shape[0]\n",
    "\n",
    "    px = sum(df[label_column]==gt_sentiment)\n",
    "    py = sum(df[word]==True)\n",
    "    pxy = len(df[(df[label_column]==gt_sentiment) & (df[word]==True)])\n",
    "\n",
    "    if pxy==0 and (px != 0 and py != 0):#Log 0 cannot happen\n",
    "        pmi = math.log((pxy+0.0001)*N/(px*py))\n",
    "    elif px == 0 or py == 0 or pxy == 0:\n",
    "        pmi = math.log(0.0001)\n",
    "    else:\n",
    "        pmi = math.log(pxy*N/(px*py))\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6975c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1177.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>1.098874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indigenous</th>\n",
       "      <td>0.867755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>police</th>\n",
       "      <td>0.843353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racist</th>\n",
       "      <td>0.779285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protest</th>\n",
       "      <td>0.724935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>0.630714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>0.59495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>0.482641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child</th>\n",
       "      <td>0.473261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>0.462087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pmi\n",
       "israel      1.098874\n",
       "indigenous  0.867755\n",
       "police      0.843353\n",
       "racist      0.779285\n",
       "protest     0.724935\n",
       "war         0.630714\n",
       "action       0.59495\n",
       "black       0.482641\n",
       "child       0.473261\n",
       "white       0.462087"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmiposdf = pmiForAllCal(combined_df,get_topk_words(df, 500),'negative')\n",
    "pmiposdf.sort_values('pmi',ascending=0).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
